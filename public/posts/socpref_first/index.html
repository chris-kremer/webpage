<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=57920&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>CK</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="&lt;!DOCTYPE html&gt;


  
  
  
  
  
  On the Generalizability of Lab Experiments
  
  



On the Generalizability of Lab Experiments
Christian Kremer">
    <meta name="generator" content="Hugo 0.139.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:57920/posts/socpref_first/">
    

    <meta property="og:url" content="http://localhost:57920/posts/socpref_first/">
  <meta property="og:site_name" content="CK">
  <meta property="og:title" content="CK">
  <meta property="og:description" content="&lt;!DOCTYPE html&gt; On the Generalizability of Lab Experiments On the Generalizability of Lab Experiments Christian Kremer">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">

  <meta itemprop="name" content="CK">
  <meta itemprop="description" content="&lt;!DOCTYPE html&gt; On the Generalizability of Lab Experiments On the Generalizability of Lab Experiments Christian Kremer">
  <meta itemprop="wordCount" content="4777">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="CK">
  <meta name="twitter:description" content="&lt;!DOCTYPE html&gt; On the Generalizability of Lab Experiments On the Generalizability of Lab Experiments Christian Kremer">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        CK
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1"></h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Christian Kremer" />
  <meta name="dcterms.date" content="2024-04-15" />
  <title>On the Generalizability of Lab Experiments</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="/Users/chris/my-website/static/styles.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">On the Generalizability of Lab Experiments</h1>
<p class="author">Christian Kremer</p>
<p class="date">2024-04-15</p>
</header>
<h2 class="unnumbered" id="introduction">Introduction</h2>
<p>This term paper was written as part of the “Social Preference” course
at Humboldt Universität zu Berlin. It addresses a relatively recent
controversy in empirical economics with wide-ranging consequences for
the way future research will be conducted. The debate centers around
publications by Steven D. Levitt and John A. List which have been
perceived as critical of laboratory setups for economic experiments.
Colin Camerer answered Levitt and List, laying out various arguments
favoring lab experiments. Camerer’s publication will be the focus of
this analysis. The first part will summarize the positions of Levitt and
List. Then, the second part will summarize Camerer’s critique of their
position before critiquing Camerer’s critique and connecting the paper
to our Social Preference course in the third and fourth parts.</p>
<h2 class="unnumbered" id="summary-of-the-levitt-list-position">Summary
of the Levitt &amp; List position</h2>
<p>Camerer seeks to address the claims made by Steven D. Levitt and John
A. List in three of their publications <span class="citation"
data-cites="Levitt.2007 Levitt.2007b Levitt.2008">(Levitt and List
2007a, 2007b, 2008)</span> (LL). He primarily focuses on one of these
papers titled: "Viewpoint: On the Generalizability of Lab Behavior to
the Field” <span class="citation" data-cites="Levitt.2007">(Levitt and
List 2007a)</span>. Hence, I, too, will focus on the arguments brought
forward in this publication. The authors describe their goal with this
work to: “summarize, in a provocative manner, some of the important
factors at work when extrapolating results from laboratory experiments
to the field.” In particular, they focus on four aspects in which the
environment in the lab typically differs from the real world and how
these differences might influence the generalizability of lab results.
To think about these four differences in a structured way, they propose
a model of decision-making. Agents are utility maximizing with a utility
function:</p>
<p><span
class="math display"><em>U</em><sub><em>i</em></sub>(<em>a</em>, <em>v</em>, <em>n</em>, <em>s</em>) = <em>M</em><sub><em>i</em></sub>(<em>a</em>, <em>v</em>, <em>n</em>, <em>s</em>) + <em>W</em><sub><em>i</em></sub>(<em>a</em>, <em>v</em>) − <em>c</em></span></p>
<p>Their utility depends on their actions (a) via two channels. One is
the wealth effect (<span
class="math inline"><em>W</em><sub><em>i</em></sub></span>), which
depends on the action and is an increasing function of the stakes of the
decision (v). The second effect is the non-monetary moral cost of the
action (<span
class="math inline"><em>M</em><sub><em>i</em></sub></span>). This effect
is a function of the selected action as well as the magnitude of the
negative impact the decision has on others (v), the set of social norms
(n), and scrutiny (s). Furthermore, in the model, cognitive costs (c)
are assumed to exist and to increase with the difficulty of
decision-making.</p>
<p>LL name four key differences between lab and reality, which are
summarized below:</p>
<h2 class="unnumbered" id="stakes">Stakes</h2>
<p>Levitt and Lists note that when the stakes of real-world situations
cannot be replicated in the lab, one can not necessarily assume lab
results to generalize to non-experimental situations. In contrast to
other disciplines, it is common for economics experiments to have some
monetary pay-out for participants depending on their choices. The model
assumes that people respond to (monetary) incentives. In the utility
equation (1), we see that the situation’s stakes (v) are a factor in
utility optimization.</p>
<h2 class="unnumbered" id="pleasing-the-experimenter">Pleasing the
experimenter</h2>
<p>Unlike in the real world, in lab experiments, participants know that
an experimenter monitors their actions. This alters the scrutiny felt in
the situation, hence changing the utility function. This systematically
different level of scrutiny would result in systematically different
actions by the participants as opposed to agents in real-world
situations.</p>
<h2 class="unnumbered" id="learning-effects">Learning effects</h2>
<p>There is a practical limitation for the duration of lab experiments.
As the utility function (1) states, decision-making is associated with
cognitive costs. For one-time decisions this cognitive cost might be too
high to justify searching for the theoretically optimal action. Still,
in real-world situations where agents are often repeatedly confronted
with similar situations, marginal cognitive costs decrease, which leads
to more optimal decision-making. Levitt and List argue that when lab
experiments cannot replicate the possibility of accumulating learning
effects, we should not expect lab results to be necessarily equivalent
to the behaviour observed in the real world.</p>
<h2 class="unnumbered" id="selection-effects">Selection effects</h2>
<p>In economics, lab experiments are typically performed in a university
setting with students as participants. The specific forms and weights of
utility functions differ from person to person. This does not inhibit
the generalizability of results as long as the utility functions of the
people tested do not differ systematically from the group one might want
to generalize to. LL state that for most questions, this assumption will
not hold. The subset of students participating in economics experiments
is significantly different from the whole student population, let alone
the population at large.</p>
<h2 class="unnumbered" id="conclusion">Conclusion</h2>
<p>List and Levitt advise caution when generalizing lab experiments for
these four main reasons. They predict that "behaviour will converge
across situations as the economically and psychologically relevant
factors converge” while warning that “relevant factors will rarely
converge across the lab and many field settings." They conclude that”at
a minimum, lab experiments can provide a crucial first understanding of
qualitative effects, suggest underlying mechanisms that might be at work
when certain data patterns are observed, provide insights into what can
happen, and evoke empirical puzzles.” <span class="citation"
data-cites="Levitt.2007">(Levitt and List 2007a)</span></p>
<h1 class="unnumbered" id="camerers-critique">Camerer’s Critique</h1>
<p>Camerer’s critique <span class="citation"
data-cites="Camerer.2011">(2011)</span> features three main arguments.
(1) Generalizability is not a main goal for lab experiments. (2) Most
features that might compromise the generalizability of lab findings,
according to Levitt &amp; List, are not unique to lab experiments, and
(3) literature shows that lab-field generalizability is often quite
good. In the following, I will examine the merit of each of those claims
and show the connections to the claims made by Levitt and List.</p>
<h2 class="unnumbered"
id="generalizability-is-not-a-primary-concern-for-lab-experiments">1.
Generalizability is not a primary concern for lab experiments</h2>
<p>Camerer proposes two viewpoints on experimental economics. The
scientific view is that “all empirical studies contribute evidence about
the general way in which [economic factors] […] influence economic
behaviour." The policy view stresses generalizability as it aims to use
the knowledge for policy actions. Camerer asserts that Levitt and List
subscribe to the policy view, while most experimentalists hold the
scientific view.</p>
<h2 class="unnumbered"
id="field-experiments-suffer-from-the-same-flaws-in-generalizability">2.
Field experiments suffer from the same flaws in generalizability</h2>
<p>YCamerer’s second main argument asserts that factors that might limit
the generalizability of lab experiments to the field also create
problems for generalizing from field results to other field applications
(2.1). He further states that all factors (except for obtrusive
observation) are not necessarily part of lab experiments and do not
necessarily impact generalizability to the field (2.2).</p>
<h2 class="unnumbered"
id="the-empirical-evidence-for-differences-in-lab-and-field-is-weak">3.
The empirical evidence for differences in lab and field is weak</h2>
<p>This argument has three parts that all engage with the current
literature on generalizability. First, the initial study that sought to
create similar setups for experiments both in the field and the lab to
compare results <span class="citation" data-cites="List.2006">(John A.
List 2006)</span> observed significant differences in lab and field
behaviour. Camerer claims this finding to not be statistically reliable
based on new, previously unreported analysis. (3.1) Second, other
experiments that try to create similar situations to compare behaviour
in the lab with behaviour in the field include just one study that gives
conclusive evidence in favor of differences in behaviour. (3.2) Third,
for papers that compare lab and field results without closely matching
the circumstances, more than 20 studies find good comparability, while
only 2 find very different results in lab and field. (3.3)</p>
<h1 class="unnumbered" id="validity-of-critiques">Validity of
Critiques</h1>
<p>(1) Arguing LL do not adhere to the scientific view described by
Camerer misses the core of their argument. In their writings, Levitt and
List do not entertain the thought that external validity should be a
prerequisite for lab experiments or that experiments aiming to find
general principles not applicable in natural environments should not
exist. They provide a model to consider which factors in the
experimental setup might promote or inhibit generalizability to the
field. Their writings do not imply that every experiment has to have
perfect external validity. It is unclear if Camerer thinks contemplating
external validity is really "distracting" and should be avoided, as it
is a net negative for scientific progress. We may assume this is not the
case, as a quick search of his publications shows him frequently
contemplating external validity of his experiments.<a href="#fn1"
class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a><br />
(2.1) When arguing that field-field generalizability is similarly
problematic to lab-field generalizability, <span class="citation"
data-cites="Camerer.2011">(C. Camerer 2011)</span> presents an example
regarding dictator games and charitable giving. Dictator games are a
widely used setup in lab experiments, the results of which show
substantial selfless giving. These results have been criticized as they
seem to be at odds with the much lower levels of charitable giving
observed in the real world. He rejects that critique by arguing that
even though people might have interpreted the results of dictator game
lab experiments as altruism, comparing these results with charitable
giving of earned income in the real world was never reasonable. He then
argues in favor of lab experiments: "The nature of entitlements,
deservingness, stakes, and obtrusiveness […] can all be controlled much
more carefully than in most field settings”. This is true and supports
the point LL make. Factors like stakes (v) and scrutiny (s) should be
actively considered when setting up lab experiments and generalizing
them from the lab to the field. Suppose one is trying to observe social
preferences for charitable giving in the real world (as Camerer assumes
in this example). In that case, a lab experiment is unlikely to give a
useful result as factors like the level of scrutiny might be varied in
the lab. However, this does not provide any information about the
quantitative results one can expect in the real world as the real-world
level of scrutiny is unknown. Hence, only field experiments could
generate results that predict further field behaviour, even if the
generalizability is quite narrow.<br />
(2.1.1) On scrutiny specifically, Camerer make some arguments, which
serve as useful example of problems in other parts of the paper. For the
factor of scrutiny to be impactful, Camerer argues subjects would have
to “(a) have a view of what hypothesis the experimenter favors
(or”demands"); and (b) be willing to sacrifice money to help prove the
experimenter’s hypothesis.” He goes on to argue that “condition (a) is
just is [sic] not likely to hold because subjects have no consistent
view about what the experimenter expects." If we accept the proposition
that subjects’ views are inconsistent for the sake of argument, they
could still impact the results. Going back to the example of dictator
games, imagine no subject has any intrinsic desire to give in a dictator
game; 40% of participants think the experimenter expects them to give
half their endowment, while 60% believe the experimenter expects them to
give nothing. In this case, the participants have inconsistent views of
what the experimenter might expect, and still, a minority of
participants would skew the results dramatically. This setup would
explain the observed giving in dictator games without the need for
consistent views of the experimenter’s expectation. Hence, condition (a)
does not have to hold for scrutiny to affect the results of an
experiment. The argumentation is reminiscent of the popular debate
tactic of "false premise setting." The argument focuses on whether
expectations of experimenter demand are consistent across participants
while pretending that consistency is required to change the results,
which it is not. This specific form of straw-manning is emblematic of
Camerer’s persistent argument against the worthlessness of lab
experiments, while this is a much more extreme and less nuanced claim
than any argument written by LL.<br />
<br />
(2.1.2) Regarding condition (b), Camerer argues that if subjects prefer
to fulfill experimenters’ expectations, the effect will shrink with
increasing stakes. He cites <span class="citation"
data-cites="Camerer.1999c">C. F. Camerer and Hogarth (1999)</span>,
arguing that raising stakes has little effect. After extensively
studying the paper, I found that most of the analyzed papers study the
effect of increased financial incentives on the performance of cognitive
or physical tasks. The difference from the example of the dictator game
is that in these cases, the motivation to do well to please the
experimenter and the motivation to do well to earn more money are in
line with each other, as opposed to the dictator game where pleasing the
experimenter might come at the cost of personal financial gain. These
experiments do not indicate if the increase in financial rewards for
good performance reduces the impact of the preference to please the
experimenter. In fact, among the 74 papers considered by Camerer and
Hogarth, two observe the change in behaviour through increased financial
stakes in a dictator game. Both find significantly less social giving
with increased stakes <span class="citation"
data-cites="Forsythe.1994 Sefton.1992">(Forsythe et al. 1994; Sefton
1992)</span>. These papers show that in situations where performance and
experimenter expectations are aligned increasing financial rewards often
reduces performance. This result is explained by the financial rewards
reframing the situation and crowing out the (stronger) intrinsic
motivation but could also be explained by the financial reward reframing
the situation and crowding out the preference to please the
experimenter. With increasing rewards, the preference to please the
experimenter might decline, making these results artefacts of laboratory
conditions. To summarize, the cited meta-analysis is not only
inapplicable in large parts, but the small subset of papers analysed
that speak to Camerer’s argument explicitly contradict his thesis,
showing the exact opposite of what his thesis would predict. Beyond
that, taking experimenter demand effects seriously calls into question
the interpretation of the whole meta-analysis, as they provide a
competing explanation for intrinsic motivation. This example of a quite
selective reading of the literature is especially egregious, considering
he is the lead author of the paper cited, and thus deserves to be
addressed in more detail.<br />
<br />
(2.2) It might technically be true that the aspects of lab design LL
criticize are not necessary components of such, but even so, it is
hardly a critique of LL’s argument. Camerer himself describes what he
calls the”common design” of lab experiments as follows: “Typically
behavior is observed obtrusively, decisions are described abstractly,
subjects are self-selected volunteers from convenience samples (e.g.,
college students), and per-hour financial incentives are modest.” Even
if those characteristics are not by definition linked to lab
experiments, they are the current standard and part of the vast majority
of experiments, making Camerer’s thesis theoretically valid but
pragmatically ineffectual. LL argues that those characteristics create
problems in generalizability and explicitly promote the creation of lab
experiments whose characteristics fit closer to the real world. They do
not argue that lab experiments are inherently bad but raise awareness
for specific factors in lab design that might practically impact
generalizability.<br />
(3.1) We established that thinking about external validity can be
worthwhile and that factors like the existence of an experimenter, the
added level of scrutiny, and the atypical demography of participants, as
well as the generally low stakes, are factors that might reasonably be
considered when generalizing from the lab to the field. The literature
indicating how large the differences between lab and field results might
be. Camerer focuses on a single paper which tried to create analogous
experiments in a lab and field setting to observe differences in
behaviour <span class="citation" data-cites="List.2006">(John A. List
2006)</span>. After requesting a re-examining of List’s data, Camerer
claims two new findings. The experiment observes the interaction of
buyers and sellers of playing cards, both in the lab and the field.
Buyers are instructed to go to sellers and request the best possible
card for a determined price. He claims that the appropriate variable to
focus on is the difference in price sensitivity for non-local traders in
the lab and the field. That is the difference in offered card quality
for a given increase in offered price. As his first new finding, Camerer
observes these effects as not being statistically different in both
settings. <span class="citation" data-cites="AlUbaydli.2013">Al-Ubaydli
and List (2013)</span> argue that this was not the study’s focus, which
tried to observe gift-giving, not reciprocity. This seems correct to me
but misses the larger point. Camerer argues that one should not
generalize to a field setting but to the general behaviour function,
which he assumes to be parallel in lab and field. LL reject this
assumption and argue that some characteristics of lab experiments
influence behaviour in specific and biased ways that do not occur
outside the lab, making lab experiments less suitable for generalization
to the general behaviour function governing behaviour in all situations.
In LL’s view, lab results give us information about human behaviour in
labs but not necessarily much else as long as we do not specifically
engage with the differences between the lab and every other setting. So
what do the results of <span class="citation"
data-cites="List.2006">John A. List (2006)</span> show? Are there
behaviour differences in the lab and field? Yes, quite a few.
Gift-giving is only observed in the lab, not the field. The impact of
sellers being local or foreign differs systematically between settings.
We should be aware of the burden of proof required to support the thesis
that lab and field data do not vary systematically. Given a significance
level (say 5%), one would need to show that less than 5% of results in
comparisons between lab and field show significant differences. Camerer
does not provide a systematic account of that (which might be difficult
due to the lack of sample size), but his unsystematic list of results is
not sufficient to support this claim. To illustrate the point, I
requested the raw data from Prof. List, added a dummy variable for lab
settings, and ran an OLS-regression analysis on the data from the direct
comparison setup between lab and field. This very simple analysis shows
whether the setting factor plays a significant role. The results imply
it does - at a 1% significance level.<br />
<br />
(3.2) &amp; (3.3) The final part of Camerer’s analysis does not directly
respond to LL but surveys the literature to assess if LL’s theoretical
concerns make a difference in practice. Beyond <span class="citation"
data-cites="List.2006">(John A. List 2006)</span>, Camerer identifies
six studies comparing lab and field setups directly and more than 20
studies comparing field setups with vaguely similar lab experiments. He
reports to finding only one study with differing results in the closely
matched setups and two studies with differing results for the less
closely matched comparisons. These observations lead him to conclude
that lab results produce data that reliably coincides with field
findings, calling into question the warnings about lab generalizability.
This conclusion suffers from two flaws we previously discussed before.
The first of which is the selective reading of the literature (similar
to section 2.1.2). The first paper Camerer describes to be a close match
between lab and field is <span class="citation"
data-cites="list2009">(John A. List 2009)</span>, which finds cheating
behaviour to be more common in a field setting as opposed to the
corresponding lab setting. Camerer dismissed this finding as
statistically insignificant, pointing out that explicit reporting on
significance level was missing. This is indeed correct; in contrast to
the more central finding of the paper, List gave only point estimates
comparing the lab and field behaviour. However, just like for <span
class="citation" data-cites="List.2006">(John A. List 2006)</span>, the
raw data is available on request and an OLS-regression estimating the
effect of the setting show significant results at the 5 Similar to this
paper, many of the results Camerer dismisses as not showing significant
differences, appear to give much stronger evidence for differing results
than Camerer’s characterisation of them indicates.</p>
<p>The second flaw in this conclusion is that it implies the burden of
proof should be on the side warning about generalizability of lab
experiments, while it should be on the one defending it. Even if only
one in six closely matched setups produce different results between lab
and field, this would be a reason to be cautious about generalizing
without explicitly addressing potential significant differences between
lab and field that might influence results. Level effects and
differences in effect size can be very important for policy
consideration.</p>
<h1 class="unnumbered" id="missing-critiques">Missing critiques</h1>
<p>Beyond the problems with the critiques Camerer has brought forth,
there were also some critiques that should have been made but were not.
The most prominent among them is LL’s reliance on lab results to
motivate their decision-making model. The results leading to the
inclusion of variables like scrutiny or stakes in the model came not
from the field, or a field-lab comparison, but from the lab. One example
of this is the factor of scrutiny and the closely connected idea of
experimenter demand effects. LL’s argument for including scrutiny in the
utility function leans on the work of <span class="citation"
data-cites="orne1959demand orne1959nature orne19621962">(Orne 1959a,
1959b, 1962)</span>. Orne showed that altering experimental settings in
a way that increases participants awareness of the experimenters’
preferences leads to behaviour that’s more in line with experimenters
demands. LL demonstrate by their argumentation, that this lab result is
significant in and of itself. It does not need field validation, and
existence of the effect itself is the valuable information, it’s size.
Many of such cases exist, where lab results alone have the capacity to
move science forward in valuable, practical ways.</p>
<h1 class="unnumbered" id="links-to-the-course">Links to the course</h1>
<p><span class="citation" data-cites="Camerer.2011">C. Camerer
(2011)</span> and <span class="citation" data-cites="Levitt.2007">Levitt
and List (2007a)</span> are highly connected to various parts of the
course. The most obvious connection is the discussion of general
criticisms of lab experiments, which heavily features both the LL paper
and the Camerer paper as a response. In addition to addressing LL and
Camerer directly, Chapter 6 presented various studies relating to
problems LL addressed.</p>
<h2 class="unnumbered" id="connections-to-ll">Connections to LL</h2>
<p>First, the lecture addresses the work of <span class="citation"
data-cites="Hoffman.1996">Hoffman, McCabe, and Smith (1996)</span>,
which explores the impact of scrutiny on behaviour in dictator games and
shows that reducing the scrutiny in dictator games significantly
decreases giving. This is linked to the concept of scrutiny in LL and
experimenter demand effects, which LL cite as a potential problem in lab
experiments. Closely related to these results, <span class="citation"
data-cites="Berg.1995">(Berg, Dickhaut, and McCabe 1995)</span> show
that the impact of reducing scrutiny is much weaker in trust games.
Another piece of literature the course presents on experimenter demand
effects is the work by <span class="citation"
data-cites="Bardsley.2008">Bardsley (2008)</span> , which studies how
obfuscating the experiment’s aim also leads to less giving behaviour.
This, too, serves as evidence for significant experimenter demand
effects. Another criticism by LL regarding lab experiments is how their
setup is often quite different from real-world situations; this might
include the scrutiny and potential differences in behaviour due to the
participants being endowed with money instead of risking their own
money. Specifically, receiving an endowment from an experimenter might
be associated with different social norms than deciding about one’s own
money (“n” in the model). In the course, we learned about the work of
<span class="citation" data-cites="Cherry.2002">Cherry, Frykblom, and
Shogren (2002)</span> on this topic. <span class="citation"
data-cites="Cherry.2002">Cherry, Frykblom, and Shogren (2002)</span> let
one group of dictators work for their money and compare their giving
behaviour with that of dictators receiving an endowment. They found a
sharp decrease in giving behaviour when dictators had to work for their
money. Under the lens of LL’s work, these results show significant
differences in perceived social norms between lab experiments where
participants receive an endowment and other situations in which people
decide about their own money. Another key component of LL’s critique is
the pool of participants used in lab experiments. They argue that
differences between the demography of participants and that of the
people whose real-world actions one tries to predict may lead to biased
results. This topic was extensively covered in chapter 8 of the course.
First, we were introduced to the work of <span class="citation"
data-cites="Roth.1991">Roth et al. (1991)</span> examining the
cross-country differences in behaviours in ultimatum games, finding
modest differences in both offers and acceptance rates. Building on
that, <span class="citation" data-cites="Henrich.2001">Henrich et al.
(2001)</span> performed a variety of games (including ultimatum games)
with various small-scale traditional societies, finding more pronounced
differences in behaviour. Offers are, on average, lower and the
acceptance rate higher, which yields offers higher than
profit-maximizing behaviour. Results of dictator and public good games
also showed significant differences from the usual student samples.
Further evidence for the significance of cultural differences is
provided by <span class="citation" data-cites="Herrmann.2008">Herrmann,
Thoni, and Gachter (2008)</span>, who show different reactions to
punishment across cultures. In addition to cross-cultural/country
differences, there may be systematic differences in the behaviour of
various demographic groups within a country. Many experiments are
performed with university students; <span class="citation"
data-cites="Cappelen.2015">Cappelen et al. (2015)</span> examined
whether students’ behaviour differs significantly from the general
population’s. They found students to be less prosocial and exhibiting
smaller gender differences. Furthermore, motives like efficiency,
equality, or reciprocity differed significantly from the population.
This literature on the impact of culture and demography on behaviour
aligns with LL’s warnings concerning the generalizability of results
obtained from one group (e.g., students in Western countries) to
others.</p>
<h2 class="unnumbered" id="connections-to-camerer">Connections to
Camerer</h2>
<p>The first part of Camerer’s critique develops the concepts of the
scientific view and the policy view on economic research and accuses LL
of (wrongly) taking a policy view, which argues for the importance of
deriving real-world predictions from the research. This difference
between LL and Camerer speaks directly to the question raised in lecture
10: "But do they [social-preference models] help economics or economic
policy?" This is connected to the first critique, where Camerer
described this question as "distracting". The course gives examples of
how models, including social preferences, can make different predictions
for outcomes than neo-classical models, (e.g. <span class="citation"
data-cites="ReyBiel.2008">Rey–Biel (2008)</span>; <span class="citation"
data-cites="Dufwenberg.2011">Dufwenberg, Gächter, and Hennig-Schmidt
(2011)</span>), and how this might affect predicted policy outcomes.</p>
<h1 class="unnumbered" id="acknowledgments">Acknowledgments</h1>
<p>I thank Prof. List for kindly providing the raw data for <span
class="citation" data-cites="List.2006 list2009">(John A. List 2006;
John A. List 2009)</span> . Data supporting this study’s findings are
available upon reasonable request to the corresponding author.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-AlUbaydli.2013" class="csl-entry" role="listitem">
Al-Ubaydli, Omar, and John A. List. 2013. <span>“On the Generalizability
of Experimental Results in Economics: With a Response to
Camerer.”</span> <span>National Bureau of Economic Research</span>.
</div>
<div id="ref-Bardsley.2008" class="csl-entry" role="listitem">
Bardsley, Nicholas. 2008. <span>“Dictator Game Giving: Altruism or
Artefact?”</span> <em>Experimental Economics</em> 11: 122–33.
</div>
<div id="ref-Berg.1995" class="csl-entry" role="listitem">
Berg, Joyce, John Dickhaut, and Kevin McCabe. 1995. <span>“Trust,
Reciprocity, and Social History.”</span> <em>Games and Economic
Behavior</em> 10 (1): 122–42.
</div>
<div id="ref-Camerer.2011" class="csl-entry" role="listitem">
Camerer, Colin. 2011. <span>“The Promise and Success of Lab-Field
Generalizability in Experimental Economics: A Critical Reply to Levitt
and List.”</span> <em>Available at SSRN 1977749</em>.
</div>
<div id="ref-Camerer.1999c" class="csl-entry" role="listitem">
Camerer, Colin F., and Robin M. Hogarth. 1999. <span>“The Effects of
Financial Incentives in Experiments: A Review and
Capital-Labor-Production Framework.”</span> <em>Journal of Risk and
Uncertainty</em> 19: 7–42.
</div>
<div id="ref-Camerer.1999" class="csl-entry" role="listitem">
Camerer, Colin, and Dan Lovallo. 1999. <span>“Overconfidence and Excess
Entry: An Experimental Approach.”</span> <em>American Economic
Review</em> 89 (1): 306–18.
</div>
<div id="ref-Cappelen.2015" class="csl-entry" role="listitem">
Cappelen, Alexander W., Knut Nygaard, Erik Ø. Sørensen, and Bertil
Tungodden. 2015. <span>“Social Preferences in the Lab: A Comparison of
Students and a Representative Population.”</span> <em>The Scandinavian
Journal of Economics</em> 117 (4): 1306–26.
</div>
<div id="ref-Cherry.2002" class="csl-entry" role="listitem">
Cherry, Todd L., Peter Frykblom, and Jason F. Shogren. 2002.
<span>“Hardnose the Dictator.”</span> <em>American Economic Review</em>
92 (4): 1218–21.
</div>
<div id="ref-Dufwenberg.2011" class="csl-entry" role="listitem">
Dufwenberg, Martin, Simon Gächter, and Heike Hennig-Schmidt. 2011.
<span>“The Framing of Games and the Psychology of Play.”</span>
<em>Games and Economic Behavior</em> 73 (2): 459–78.
</div>
<div id="ref-Forsythe.1994" class="csl-entry" role="listitem">
Forsythe, Robert, Joel L. Horowitz, Nathan E. Savin, and Martin Sefton.
1994. <span>“Fairness in Simple Bargaining Experiments.”</span>
<em>Games and Economic Behavior</em> 6 (3): 347–69.
</div>
<div id="ref-Henrich.2001" class="csl-entry" role="listitem">
Henrich, Joseph, Robert Boyd, Samuel Bowles, Colin Camerer, Ernst Fehr,
Herbert Gintis, and Richard McElreath. 2001. <span>“In Search of Homo
Economicus: Behavioral Experiments in 15 Small-Scale Societies.”</span>
<em>American Economic Review</em> 91 (2): 73–78.
</div>
<div id="ref-Herrmann.2008" class="csl-entry" role="listitem">
Herrmann, Benedikt, Christian Thoni, and Simon Gachter. 2008.
<span>“Antisocial Punishment Across Societies.”</span> <em>Science</em>
319 (5868): 1362–67.
</div>
<div id="ref-Hoffman.1996" class="csl-entry" role="listitem">
Hoffman, Elizabeth, Kevin McCabe, and Vernon L. Smith. 1996.
<span>“Social Distance and Other-Regarding Behavior in Dictator
Games.”</span> <em>The American Economic Review</em> 86 (3): 653–60.
</div>
<div id="ref-Levitt.2007" class="csl-entry" role="listitem">
Levitt, Steven D., and John A. List. 2007a. <span>“On the
Generalizability of Lab Behaviour to the Field.”</span> <em>Canadian
Journal of Economics/Revue Canadienne d’<span>é</span>conomique</em> 40
(2): 347–70.
</div>
<div id="ref-Levitt.2007b" class="csl-entry" role="listitem">
———. 2007b. <span>“What Do Laboratory Experiments Measuring Social
Preferences Reveal about the Real World?”</span> <em>Journal of Economic
Perspectives</em> 21 (2): 153–74.
</div>
<div id="ref-Levitt.2008" class="csl-entry" role="listitem">
———. 2008. <span>“Homo Economicus Evolves.”</span> <em>Science</em> 319
(5865): 909–10.
</div>
<div id="ref-list2009" class="csl-entry" role="listitem">
List, John A. 2009. <span>“The Economics of Open Air Markets.”</span>
National Bureau of Economic Research.
</div>
<div id="ref-List.2006" class="csl-entry" role="listitem">
List, John A. 2006. <span>“The Behavioralist Meets the Market: Measuring
Social Preferences and Reputation Effects in Actual
Transactions.”</span> <em>Journal of Political Economy</em> 114 (1):
1–37.
</div>
<div id="ref-orne1959demand" class="csl-entry" role="listitem">
Orne, Martin T. 1959a. <span>“The Demand Characteristics of an
Experimental Design and Their Implications.”</span> <em>American
Psychological Association, Cincinnati</em>.
</div>
<div id="ref-orne1959nature" class="csl-entry" role="listitem">
———. 1959b. <span>“The Nature of Hypnosis: Artifact and Essence.”</span>
<em>The Journal of Abnormal and Social Psychology</em> 58 (3): 277.
</div>
<div id="ref-orne19621962" class="csl-entry" role="listitem">
———. 1962. <span>“" on the Social Psychology of the Psychological
Experiment: With Particular Reference to Demand Characteristics and
Their Implications", American Psychologist 17: 776-783.”</span>
</div>
<div id="ref-ReyBiel.2008" class="csl-entry" role="listitem">
Rey–Biel, Pedro. 2008. <span>“Inequity Aversion and Team
Incentives.”</span> <em>Scandinavian Journal of Economics</em> 110 (2):
297–320.
</div>
<div id="ref-Roth.1991" class="csl-entry" role="listitem">
Roth, Alvin E., Vesna Prasnikar, Masahiro Okuno-Fujiwara, and Shmuel
Zamir. 1991. <span>“Bargaining and Market Behavior in Jerusalem,
Ljubljana, Pittsburgh, and Tokyo: An Experimental Study.”</span> <em>The
American Economic Review</em>, 1068–95.
</div>
<div id="ref-Sefton.1992" class="csl-entry" role="listitem">
Sefton, Martin. 1992. <span>“Incentives in Simple Bargaining
Games.”</span> <em>Journal of Economic Psychology</em> 13 (2): 263–76.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>One of many examples is his work on “reference group
neglect” in <span class="citation" data-cites="Camerer.1999">(C. Camerer
and Lovallo 1999)</span>, where he writes extensively on the real-world
implications of lab findings.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:57920/" >
    &copy;  CK 2024 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>

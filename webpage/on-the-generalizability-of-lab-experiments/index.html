<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>On the Generalizability of Lab Experiments | CK</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Introduction
This term paper was written as part of the &ldquo;Social Preference&rdquo; course at
Humboldt Universität zu Berlin. It addresses a relatively recent
controversy in empirical economics with wide-ranging consequences for
the way future research will be conducted. The debate centers around
publications by Steven D. Levitt and John A. List which have been
perceived as critical of laboratory setups for economic experiments.
Colin Camerer answered Levitt and List, laying out various arguments
favoring lab experiments. Camerer&rsquo;s publication will be the focus of
this analysis. The first part will summarize the positions of Levitt and
List. Then, the second part will summarize Camerer&rsquo;s critique of their
position before critiquing Camerer&rsquo;s critique and connecting the paper
to our Social Preference course in the third and fourth parts.">
    <meta name="generator" content="Hugo 0.139.2">
    
    
    
      <meta name="robots" content="index, follow">
    
    
      <meta name="author" content = "Christian Kremer">
    

    
<link rel="stylesheet" href="../../webpage/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://chris-kremer.github.io/webpage/webpage/on-the-generalizability-of-lab-experiments/">
    

    <meta property="og:url" content="https://chris-kremer.github.io/webpage/webpage/on-the-generalizability-of-lab-experiments/">
  <meta property="og:site_name" content="CK">
  <meta property="og:title" content="On the Generalizability of Lab Experiments">
  <meta property="og:description" content="Introduction This term paper was written as part of the “Social Preference” course at Humboldt Universität zu Berlin. It addresses a relatively recent controversy in empirical economics with wide-ranging consequences for the way future research will be conducted. The debate centers around publications by Steven D. Levitt and John A. List which have been perceived as critical of laboratory setups for economic experiments. Colin Camerer answered Levitt and List, laying out various arguments favoring lab experiments. Camerer’s publication will be the focus of this analysis. The first part will summarize the positions of Levitt and List. Then, the second part will summarize Camerer’s critique of their position before critiquing Camerer’s critique and connecting the paper to our Social Preference course in the third and fourth parts.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-04-15T00:00:00+00:00">

  <meta itemprop="name" content="On the Generalizability of Lab Experiments">
  <meta itemprop="description" content="Introduction This term paper was written as part of the “Social Preference” course at Humboldt Universität zu Berlin. It addresses a relatively recent controversy in empirical economics with wide-ranging consequences for the way future research will be conducted. The debate centers around publications by Steven D. Levitt and John A. List which have been perceived as critical of laboratory setups for economic experiments. Colin Camerer answered Levitt and List, laying out various arguments favoring lab experiments. Camerer’s publication will be the focus of this analysis. The first part will summarize the positions of Levitt and List. Then, the second part will summarize Camerer’s critique of their position before critiquing Camerer’s critique and connecting the paper to our Social Preference course in the third and fourth parts.">
  <meta itemprop="datePublished" content="2024-04-15T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-04-15T00:00:00+00:00">
  <meta itemprop="wordCount" content="4151">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="On the Generalizability of Lab Experiments">
  <meta name="twitter:description" content="Introduction This term paper was written as part of the “Social Preference” course at Humboldt Universität zu Berlin. It addresses a relatively recent controversy in empirical economics with wide-ranging consequences for the way future research will be conducted. The debate centers around publications by Steven D. Levitt and John A. List which have been perceived as critical of laboratory setups for economic experiments. Colin Camerer answered Levitt and List, laying out various arguments favoring lab experiments. Camerer’s publication will be the focus of this analysis. The first part will summarize the positions of Levitt and List. Then, the second part will summarize Camerer’s critique of their position before critiquing Camerer’s critique and connecting the paper to our Social Preference course in the third and fourth parts.">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="../../" class="f3 fw2 hover-white no-underline white-90 dib">
      
        CK
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">On the Generalizability of Lab Experiments</h1>
      
      <p class="tracked">
        By <strong>Christian Kremer</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-04-15T00:00:00Z">April 15, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="introduction" class="unnumbered">Introduction</h2>
<p>This term paper was written as part of the &ldquo;Social Preference&rdquo; course at
Humboldt Universität zu Berlin. It addresses a relatively recent
controversy in empirical economics with wide-ranging consequences for
the way future research will be conducted. The debate centers around
publications by Steven D. Levitt and John A. List which have been
perceived as critical of laboratory setups for economic experiments.
Colin Camerer answered Levitt and List, laying out various arguments
favoring lab experiments. Camerer&rsquo;s publication will be the focus of
this analysis. The first part will summarize the positions of Levitt and
List. Then, the second part will summarize Camerer&rsquo;s critique of their
position before critiquing Camerer&rsquo;s critique and connecting the paper
to our Social Preference course in the third and fourth parts.</p>
<h2 id="summary-of-the-levitt-list-position" class="unnumbered">Summary of the Levitt &amp; List position</h2>
<p>Camerer seeks to address the claims made by Steven D. Levitt and John A.
List in three of their publications
[@Levitt.2007; @Levitt.2007b; @Levitt.2008] (LL). He primarily focuses
on one of these papers titled: &quot;Viewpoint: On the Generalizability of
Lab Behavior to the Field&quot; [@Levitt.2007]. Hence, I, too, will focus on
the arguments brought forward in this publication. The authors describe
their goal with this work to: &ldquo;summarize, in a provocative manner, some
of the important factors at work when extrapolating results from
laboratory experiments to the field.&rdquo; In particular, they focus on four
aspects in which the environment in the lab typically differs from the
real world and how these differences might influence the
generalizability of lab results. To think about these four differences
in a structured way, they propose a model of decision-making. Agents are
utility maximizing with a utility function:</p>
<p>$$U_i(a, v, n, s) = M_i(a, v, n, s) + W_i(a, v) - c$$</p>
<p>Their utility depends on their actions (a) via two channels. One is the
wealth effect ($W_i$), which depends on the action and is an increasing
function of the stakes of the decision (v). The second effect is the
non-monetary moral cost of the action ($M_i$). This effect is a function
of the selected action as well as the magnitude of the negative impact
the decision has on others (v), the set of social norms (n), and
scrutiny (s). Furthermore, in the model, cognitive costs (c) are assumed
to exist and to increase with the difficulty of decision-making.</p>
<p>LL name four key differences between lab and reality, which are
summarized below:</p>
<h2 id="stakes" class="unnumbered">Stakes</h2>
<p>Levitt and Lists note that when the stakes of real-world situations
cannot be replicated in the lab, one can not necessarily assume lab
results to generalize to non-experimental situations. In contrast to
other disciplines, it is common for economics experiments to have some
monetary pay-out for participants depending on their choices. The model
assumes that people respond to (monetary) incentives. In the utility
equation (1), we see that the situation&rsquo;s stakes (v) are a factor in
utility optimization.</p>
<h2 id="pleasing-the-experimenter" class="unnumbered">Pleasing the experimenter</h2>
<p>Unlike in the real world, in lab experiments, participants know that an
experimenter monitors their actions. This alters the scrutiny felt in
the situation, hence changing the utility function. This systematically
different level of scrutiny would result in systematically different
actions by the participants as opposed to agents in real-world
situations.</p>
<h2 id="learning-effects" class="unnumbered">Learning effects</h2>
<p>There is a practical limitation for the duration of lab experiments. As
the utility function (1) states, decision-making is associated with
cognitive costs. For one-time decisions this cognitive cost might be too
high to justify searching for the theoretically optimal action. Still,
in real-world situations where agents are often repeatedly confronted
with similar situations, marginal cognitive costs decrease, which leads
to more optimal decision-making. Levitt and List argue that when lab
experiments cannot replicate the possibility of accumulating learning
effects, we should not expect lab results to be necessarily equivalent
to the behaviour observed in the real world.</p>
<h2 id="selection-effects" class="unnumbered">Selection effects</h2>
<p>In economics, lab experiments are typically performed in a university
setting with students as participants. The specific forms and weights of
utility functions differ from person to person. This does not inhibit
the generalizability of results as long as the utility functions of the
people tested do not differ systematically from the group one might want
to generalize to. LL state that for most questions, this assumption will
not hold. The subset of students participating in economics experiments
is significantly different from the whole student population, let alone
the population at large.</p>
<h2 id="conclusion" class="unnumbered">Conclusion</h2>
<p>List and Levitt advise caution when generalizing lab experiments for
these four main reasons. They predict that &quot;behaviour will converge
across situations as the economically and psychologically relevant
factors converge&quot; while warning that &ldquo;relevant factors will rarely
converge across the lab and many field settings.&quot; They conclude that&quot;at
a minimum, lab experiments can provide a crucial first understanding of
qualitative effects, suggest underlying mechanisms that might be at work
when certain data patterns are observed, provide insights into what can
happen, and evoke empirical puzzles.&rdquo; [@Levitt.2007]</p>
<h1 id="camerers-critique" class="unnumbered">Camerer&rsquo;s Critique</h1>
<p>Camerer&rsquo;s critique [-@Camerer.2011] features three main arguments. (1)
Generalizability is not a main goal for lab experiments. (2) Most
features that might compromise the generalizability of lab findings,
according to Levitt &amp; List, are not unique to lab experiments, and (3)
literature shows that lab-field generalizability is often quite good. In
the following, I will examine the merit of each of those claims and show
the connections to the claims made by Levitt and List.</p>
<h2 id="generalizability-is-not-a-primary-concern-for-lab-experiments" class="unnumbered">1. Generalizability is not a primary concern for lab experiments</h2>
<p>Camerer proposes two viewpoints on experimental economics. The
scientific view is that &ldquo;all empirical studies contribute evidence about
the general way in which [economic factors] [&hellip;] influence economic
behaviour.&quot; The policy view stresses generalizability as it aims to use
the knowledge for policy actions. Camerer asserts that Levitt and List
subscribe to the policy view, while most experimentalists hold the
scientific view.</p>
<h2 id="field-experiments-suffer-from-the-same-flaws-in-generalizability" class="unnumbered">2. Field experiments suffer from the same flaws in generalizability</h2>
<p>YCamerer&rsquo;s second main argument asserts that factors that might limit
the generalizability of lab experiments to the field also create
problems for generalizing from field results to other field applications
(2.1). He further states that all factors (except for obtrusive
observation) are not necessarily part of lab experiments and do not
necessarily impact generalizability to the field (2.2).</p>
<h2 id="the-empirical-evidence-for-differences-in-lab-and-field-is-weak" class="unnumbered">3. The empirical evidence for differences in lab and field is weak</h2>
<p>This argument has three parts that all engage with the current
literature on generalizability. First, the initial study that sought to
create similar setups for experiments both in the field and the lab to
compare results [@List.2006] observed significant differences in lab and
field behaviour. Camerer claims this finding to not be statistically
reliable based on new, previously unreported analysis. (3.1) Second,
other experiments that try to create similar situations to compare
behaviour in the lab with behaviour in the field include just one study
that gives conclusive evidence in favor of differences in behaviour.
(3.2) Third, for papers that compare lab and field results without
closely matching the circumstances, more than 20 studies find good
comparability, while only 2 find very different results in lab and
field. (3.3)</p>
<h1 id="validity-of-critiques" class="unnumbered">Validity of Critiques</h1>
<p>(1) Arguing LL do not adhere to the scientific view described by
Camerer misses the core of their argument. In their writings, Levitt and
List do not entertain the thought that external validity should be a
prerequisite for lab experiments or that experiments aiming to find
general principles not applicable in natural environments should not
exist. They provide a model to consider which factors in the
experimental setup might promote or inhibit generalizability to the
field. Their writings do not imply that every experiment has to have
perfect external validity. It is unclear if Camerer thinks contemplating
external validity is really &quot;distracting&quot; and should be avoided, as it
is a net negative for scientific progress. We may assume this is not the
case, as a quick search of his publications shows him frequently
contemplating external validity of his experiments.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><br>
(2.1) When arguing that field-field generalizability is similarly
problematic to lab-field generalizability, [@Camerer.2011] presents an
example regarding dictator games and charitable giving. Dictator games
are a widely used setup in lab experiments, the results of which show
substantial selfless giving. These results have been criticized as they
seem to be at odds with the much lower levels of charitable giving
observed in the real world. He rejects that critique by arguing that
even though people might have interpreted the results of dictator game
lab experiments as altruism, comparing these results with charitable
giving of earned income in the real world was never reasonable. He then
argues in favor of lab experiments: &quot;The nature of entitlements,
deservingness, stakes, and obtrusiveness [&hellip;] can all be controlled
much more carefully than in most field settings&rdquo;. This is true and
supports the point LL make. Factors like stakes (v) and scrutiny (s)
should be actively considered when setting up lab experiments and
generalizing them from the lab to the field. Suppose one is trying to
observe social preferences for charitable giving in the real world (as
Camerer assumes in this example). In that case, a lab experiment is
unlikely to give a useful result as factors like the level of scrutiny
might be varied in the lab. However, this does not provide any
information about the quantitative results one can expect in the real
world as the real-world level of scrutiny is unknown. Hence, only field
experiments could generate results that predict further field behaviour,
even if the generalizability is quite narrow.<br>
(2.1.1) On scrutiny specifically, Camerer make some arguments, which
serve as useful example of problems in other parts of the paper. For the
factor of scrutiny to be impactful, Camerer argues subjects would have
to &ldquo;(a) have a view of what hypothesis the experimenter favors
(or&quot;demands&quot;); and (b) be willing to sacrifice money to help prove the
experimenter&rsquo;s hypothesis.&rdquo; He goes on to argue that &ldquo;condition (a) is
just is [sic] not likely to hold because subjects have no consistent
view about what the experimenter expects.&quot; If we accept the proposition
that subjects&rsquo; views are inconsistent for the sake of argument, they
could still impact the results. Going back to the example of dictator
games, imagine no subject has any intrinsic desire to give in a dictator
game; 40% of participants think the experimenter expects them to give
half their endowment, while 60% believe the experimenter expects them to
give nothing. In this case, the participants have inconsistent views of
what the experimenter might expect, and still, a minority of
participants would skew the results dramatically. This setup would
explain the observed giving in dictator games without the need for
consistent views of the experimenter&rsquo;s expectation. Hence, condition (a)
does not have to hold for scrutiny to affect the results of an
experiment. The argumentation is reminiscent of the popular debate
tactic of &quot;false premise setting.&quot; The argument focuses on whether
expectations of experimenter demand are consistent across participants
while pretending that consistency is required to change the results,
which it is not. This specific form of straw-manning is emblematic of
Camerer&rsquo;s persistent argument against the worthlessness of lab
experiments, while this is a much more extreme and less nuanced claim
than any argument written by LL.<br>
<br>
(2.1.2) Regarding condition (b), Camerer argues that if subjects prefer
to fulfill experimenters&rsquo; expectations, the effect will shrink with
increasing stakes. He cites @Camerer.1999c, arguing that raising stakes
has little effect. After extensively studying the paper, I found that
most of the analyzed papers study the effect of increased financial
incentives on the performance of cognitive or physical tasks. The
difference from the example of the dictator game is that in these cases,
the motivation to do well to please the experimenter and the motivation
to do well to earn more money are in line with each other, as opposed to
the dictator game where pleasing the experimenter might come at the cost
of personal financial gain. These experiments do not indicate if the
increase in financial rewards for good performance reduces the impact of
the preference to please the experimenter. In fact, among the 74 papers
considered by Camerer and Hogarth, two observe the change in behaviour
through increased financial stakes in a dictator game. Both find
significantly less social giving with increased stakes
[@Forsythe.1994; @Sefton.1992]. These papers show that in situations
where performance and experimenter expectations are aligned increasing
financial rewards often reduces performance. This result is explained by
the financial rewards reframing the situation and crowing out the
(stronger) intrinsic motivation but could also be explained by the
financial reward reframing the situation and crowding out the preference
to please the experimenter. With increasing rewards, the preference to
please the experimenter might decline, making these results artefacts of
laboratory conditions. To summarize, the cited meta-analysis is not only
inapplicable in large parts, but the small subset of papers analysed
that speak to Camerer&rsquo;s argument explicitly contradict his thesis,
showing the exact opposite of what his thesis would predict. Beyond
that, taking experimenter demand effects seriously calls into question
the interpretation of the whole meta-analysis, as they provide a
competing explanation for intrinsic motivation. This example of a quite
selective reading of the literature is especially egregious, considering
he is the lead author of the paper cited, and thus deserves to be
addressed in more detail.<br>
<br>
(2.2) It might technically be true that the aspects of lab design LL
criticize are not necessary components of such, but even so, it is
hardly a critique of LL&rsquo;s argument. Camerer himself describes what he
calls the&quot;common design&rdquo; of lab experiments as follows: &ldquo;Typically
behavior is observed obtrusively, decisions are described abstractly,
subjects are self-selected volunteers from convenience samples (e.g.,
college students), and per-hour financial incentives are modest.&rdquo; Even
if those characteristics are not by definition linked to lab
experiments, they are the current standard and part of the vast majority
of experiments, making Camerer&rsquo;s thesis theoretically valid but
pragmatically ineffectual. LL argues that those characteristics create
problems in generalizability and explicitly promote the creation of lab
experiments whose characteristics fit closer to the real world. They do
not argue that lab experiments are inherently bad but raise awareness
for specific factors in lab design that might practically impact
generalizability.<br>
(3.1) We established that thinking about external validity can be
worthwhile and that factors like the existence of an experimenter, the
added level of scrutiny, and the atypical demography of participants, as
well as the generally low stakes, are factors that might reasonably be
considered when generalizing from the lab to the field. The literature
indicating how large the differences between lab and field results might
be. Camerer focuses on a single paper which tried to create analogous
experiments in a lab and field setting to observe differences in
behaviour [@List.2006]. After requesting a re-examining of List&rsquo;s data,
Camerer claims two new findings. The experiment observes the interaction
of buyers and sellers of playing cards, both in the lab and the field.
Buyers are instructed to go to sellers and request the best possible
card for a determined price. He claims that the appropriate variable to
focus on is the difference in price sensitivity for non-local traders in
the lab and the field. That is the difference in offered card quality
for a given increase in offered price. As his first new finding, Camerer
observes these effects as not being statistically different in both
settings. @AlUbaydli.2013 argue that this was not the study&rsquo;s focus,
which tried to observe gift-giving, not reciprocity. This seems correct
to me but misses the larger point. Camerer argues that one should not
generalize to a field setting but to the general behaviour function,
which he assumes to be parallel in lab and field. LL reject this
assumption and argue that some characteristics of lab experiments
influence behaviour in specific and biased ways that do not occur
outside the lab, making lab experiments less suitable for generalization
to the general behaviour function governing behaviour in all situations.
In LL&rsquo;s view, lab results give us information about human behaviour in
labs but not necessarily much else as long as we do not specifically
engage with the differences between the lab and every other setting. So
what do the results of @List.2006 show? Are there behaviour differences
in the lab and field? Yes, quite a few. Gift-giving is only observed in
the lab, not the field. The impact of sellers being local or foreign
differs systematically between settings. We should be aware of the
burden of proof required to support the thesis that lab and field data
do not vary systematically. Given a significance level (say 5%), one
would need to show that less than 5% of results in comparisons between
lab and field show significant differences. Camerer does not provide a
systematic account of that (which might be difficult due to the lack of
sample size), but his unsystematic list of results is not sufficient to
support this claim. To illustrate the point, I requested the raw data
from Prof. List, added a dummy variable for lab settings, and ran an
OLS-regression analysis on the data from the direct comparison setup
between lab and field. This very simple analysis shows whether the
setting factor plays a significant role. The results imply it does - at
a 1% significance level.<br>
<br>
(3.2) &amp; (3.3) The final part of Camerer&rsquo;s analysis does not directly
respond to LL but surveys the literature to assess if LL&rsquo;s theoretical
concerns make a difference in practice. Beyond [@List.2006], Camerer
identifies six studies comparing lab and field setups directly and more
than 20 studies comparing field setups with vaguely similar lab
experiments. He reports to finding only one study with differing results
in the closely matched setups and two studies with differing results for
the less closely matched comparisons. These observations lead him to
conclude that lab results produce data that reliably coincides with
field findings, calling into question the warnings about lab
generalizability. This conclusion suffers from two flaws we previously
discussed before. The first of which is the selective reading of the
literature (similar to section 2.1.2). The first paper Camerer describes
to be a close match between lab and field is [@list2009], which finds
cheating behaviour to be more common in a field setting as opposed to
the corresponding lab setting. Camerer dismissed this finding as
statistically insignificant, pointing out that explicit reporting on
significance level was missing. This is indeed correct; in contrast to
the more central finding of the paper, List gave only point estimates
comparing the lab and field behaviour. However, just like for
[@List.2006], the raw data is available on request and an OLS-regression
estimating the effect of the setting show significant results at the 5
Similar to this paper, many of the results Camerer dismisses as not
showing significant differences, appear to give much stronger evidence
for differing results than Camerer&rsquo;s characterisation of them indicates.</p>
<p>The second flaw in this conclusion is that it implies the burden of
proof should be on the side warning about generalizability of lab
experiments, while it should be on the one defending it. Even if only
one in six closely matched setups produce different results between lab
and field, this would be a reason to be cautious about generalizing
without explicitly addressing potential significant differences between
lab and field that might influence results. Level effects and
differences in effect size can be very important for policy
consideration.</p>
<h1 id="missing-critiques" class="unnumbered">Missing critiques</h1>
<p>Beyond the problems with the critiques Camerer has brought forth, there
were also some critiques that should have been made but were not. The
most prominent among them is LL&rsquo;s reliance on lab results to motivate
their decision-making model. The results leading to the inclusion of
variables like scrutiny or stakes in the model came not from the field,
or a field-lab comparison, but from the lab. One example of this is the
factor of scrutiny and the closely connected idea of experimenter demand
effects. LL&rsquo;s argument for including scrutiny in the utility function
leans on the work of [@orne1959demand; @orne1959nature; @orne19621962].
Orne showed that altering experimental settings in a way that increases
participants awareness of the experimenters&rsquo; preferences leads to
behaviour that&rsquo;s more in line with experimenters demands. LL demonstrate
by their argumentation, that this lab result is significant in and of
itself. It does not need field validation, and existence of the effect
itself is the valuable information, it&rsquo;s size. Many of such cases exist,
where lab results alone have the capacity to move science forward in
valuable, practical ways.</p>
<h1 id="links-to-the-course" class="unnumbered">Links to the course</h1>
<p>@Camerer.2011 and @Levitt.2007 are highly connected to various parts of
the course. The most obvious connection is the discussion of general
criticisms of lab experiments, which heavily features both the LL paper
and the Camerer paper as a response. In addition to addressing LL and
Camerer directly, Chapter 6 presented various studies relating to
problems LL addressed.</p>
<h2 id="connections-to-ll" class="unnumbered">Connections to LL</h2>
<p>First, the lecture addresses the work of @Hoffman.1996, which explores
the impact of scrutiny on behaviour in dictator games and shows that
reducing the scrutiny in dictator games significantly decreases giving.
This is linked to the concept of scrutiny in LL and experimenter demand
effects, which LL cite as a potential problem in lab experiments.
Closely related to these results, [@Berg.1995] show that the impact of
reducing scrutiny is much weaker in trust games. Another piece of
literature the course presents on experimenter demand effects is the
work by @Bardsley.2008 , which studies how obfuscating the experiment&rsquo;s
aim also leads to less giving behaviour. This, too, serves as evidence
for significant experimenter demand effects. Another criticism by LL
regarding lab experiments is how their setup is often quite different
from real-world situations; this might include the scrutiny and
potential differences in behaviour due to the participants being endowed
with money instead of risking their own money. Specifically, receiving
an endowment from an experimenter might be associated with different
social norms than deciding about one&rsquo;s own money (&ldquo;n&rdquo; in the model). In
the course, we learned about the work of @Cherry.2002 on this topic.
@Cherry.2002 let one group of dictators work for their money and compare
their giving behaviour with that of dictators receiving an endowment.
They found a sharp decrease in giving behaviour when dictators had to
work for their money. Under the lens of LL&rsquo;s work, these results show
significant differences in perceived social norms between lab
experiments where participants receive an endowment and other situations
in which people decide about their own money. Another key component of
LL&rsquo;s critique is the pool of participants used in lab experiments. They
argue that differences between the demography of participants and that
of the people whose real-world actions one tries to predict may lead to
biased results. This topic was extensively covered in chapter 8 of the
course. First, we were introduced to the work of @Roth.1991 examining
the cross-country differences in behaviours in ultimatum games, finding
modest differences in both offers and acceptance rates. Building on
that, @Henrich.2001 performed a variety of games (including ultimatum
games) with various small-scale traditional societies, finding more
pronounced differences in behaviour. Offers are, on average, lower and
the acceptance rate higher, which yields offers higher than
profit-maximizing behaviour. Results of dictator and public good games
also showed significant differences from the usual student samples.
Further evidence for the significance of cultural differences is
provided by @Herrmann.2008, who show different reactions to punishment
across cultures. In addition to cross-cultural/country differences,
there may be systematic differences in the behaviour of various
demographic groups within a country. Many experiments are performed with
university students; @Cappelen.2015 examined whether students&rsquo; behaviour
differs significantly from the general population&rsquo;s. They found students
to be less prosocial and exhibiting smaller gender differences.
Furthermore, motives like efficiency, equality, or reciprocity differed
significantly from the population. This literature on the impact of
culture and demography on behaviour aligns with LL&rsquo;s warnings concerning
the generalizability of results obtained from one group (e.g., students
in Western countries) to others.</p>
<h2 id="connections-to-camerer" class="unnumbered">Connections to Camerer</h2>
<p>The first part of Camerer&rsquo;s critique develops the concepts of the
scientific view and the policy view on economic research and accuses LL
of (wrongly) taking a policy view, which argues for the importance of
deriving real-world predictions from the research. This difference
between LL and Camerer speaks directly to the question raised in lecture
10: &quot;But do they [social-preference models] help economics or
economic policy?&quot; This is connected to the first critique, where
Camerer described this question as &quot;distracting&quot;. The course gives
examples of how models, including social preferences, can make different
predictions for outcomes than neo-classical models, (e.g. @ReyBiel.2008;
@Dufwenberg.2011), and how this might affect predicted policy outcomes.</p>
<h1 id="acknowledgments" class="unnumbered">Acknowledgments</h1>
<p>I thank Prof. List for kindly providing the raw data for
[@List.2006; @list2009] . Data supporting this study&rsquo;s findings are
available upon reasonable request to the corresponding author.</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>One of many examples is his work on &ldquo;reference group neglect&rdquo; in
[@Camerer.1999], where he writes extensively on the real-world
implications of lab findings.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://chris-kremer.github.io/webpage/" >
    &copy;  CK 2024 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
